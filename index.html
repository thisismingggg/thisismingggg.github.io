<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="keywords" content="harmful dataset, harmful content detection, Vision Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/denny3388">Chen Yeh</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/thisismingggg">You-Ming Chang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://walonchiu.github.io">Wei-Chen Chiu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ningyu1991.github.io/">Ning Yu</a><sup>2</sup>
            </span>
          </div>
          <span class="author-block">
            (<sup>*</sup>Both authors contribute equally)
          </span>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Yang Ming Chiao Tung University,</span>
            <span class="author-block"><sup>2</sup>Netflix Eyeline Studios</span>
          </div>
          <span class="author-block">
            &#127881 Accepted to <b>NeurIPS'24 Datasets and Benchmarks Track</b> &#127881
          </span>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- NeurIPS PDF Link. -->
              <span class="link-block">
                <!-- <a href="https://arxiv.org/pdf/2011.12948" -->
                <a class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://eva-lab.synology.me:8001/sharing/2iar2UrZs"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (VHD11K)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.19734"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (w/ Appendix)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <!-- <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" -->
                <a class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/nctu-eva-lab/VHD11K"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Brief Intro -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview Image. -->
    <div class="columns is-centered ">
      <div class="column is-four-fifths has-text-centered interpolation-panel">
        <img src="./static/images/overview.png"
              class="overview-image"
              alt="Dataset curating process image."/>
        <p>Overview: dataset curating process. Please note that the white rectangle masks serve as censorship, and are not included as inputs. <i>"A."</i>, <i>"N."</i> and <i>"J."</i> stand for the affirmative debater, the negative debater and the judge respectively.</p>
      </div>
    </div>
    <!--/ Overview Image. -->

    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">&#128161 Overview</h2>
        <div class="content has-text-justified">
          <p>
            We propose a comprehensive and extensive harmful dataset, <b>Visual Harmful Dataset 11K (VHD11K)</b>, consisting of <b>10,000 images</b> and <b>1,000 videos</b>, crawled from the Internet and generated by 4 generative models, across a total of <b>10 harmful categories</b> covering a full spectrum of harmful concepts with non-trival definition. 
          </p>
          <p>
            We also propose a novel annotation framework by formulating the annotation process as a <b>Multi-agent Visual Question Answering (VQA) Task</b>, having 3 different VLMs "<b>debate</b>" about whether the given image/video is harmful, and incorporating the in-context learning strategy in the debating process.
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">&#128249 Video</h2>
        <div class="publication-video">
          <!-- <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- VHD11K. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">&#128218 VHD11K: Our Proposed Multimodal Dataset for Visual Harmfulness Recognition</h2>
        <div class="content has-text-justified">
          <p>
            The entire dataset is publicly available at <a href="https://eva-lab.synology.me:8001/sharing/2iar2UrZs">here</a>. Under the shared folder, there are: 
          </p>
          <pre><code>dataset_10000_1000
|--croissant-vhd11k.json            # metadata of VHD11K
|--harmful_image_10000_ann.json     # annotaion file of harmful images of VHD11K 
                                      (image name, harmful type, arguments, ...)
|--harmful_images_10000.zip         # 10000 harmful images of VHD11K
|--harmful_video_1000_ann.json      # annotaion file of harmful videos of VHD11K
                                      (video name, harmful type, arguments, ...)
|--harmful_videos_1000.zip          # 1000 harmful videos of VHD11K
|--ICL_samples.zip                  # in-context learning samples used in annoators
    |--ICL_images                   # in-context learning images
    |--ICL_videos_frames            # frames of each in-context learning video</code></pre>
        </div>
      </div>
    </div>
    <!--/ VHD11K. -->

    <!-- Evaluation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">&#128202 Evaluation</h2>
        <div class="columns is-centered">
          <div class="column is-four has-text-centered">
            <img src="./static/images/benchmarking.png"
                     class="interpolation-image"
                     alt="Pretrained models banchmarking table."/>
            <p>Harmfulness detection accuracies of <b><u>pretrained</u></b> baseline methods.</p>
          </div>
          <div class="column is-four has-text-centered">

            <img src="./static/images/VHD11K_vs_SMID.png"
                  class="interpolation-image"
                  alt="VHD11K and SMID comparison table."/>
            
            <p>Harmfulness detection accuracies of <b><u>prompt-tuned methods on VHD11K and SMID</u></b>.</p>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            Evaluation and experimental results demonstrate that 
            <ol>
              <li>
                the <b>great alignment</b> between the annotation from our novel annotation framework and those from human, ensuring the reliability of VHD11K.
              </li>
              <li>
                our full-spectrum harmful dataset <b>successfully identifies the inability of existing harmful content detection methods</b> to detect extensive harmful contents and improves the performance of existing harmfulness recognition methods.
              </li>
              <li>
                our dataset <b>outperforms the baseline dataset, SMID,</b> as evidenced by the superior improvement in harmfulness recognition methods.
              </li>
            </ol>
          </p>
        </div>
      </div>
    </div>
    <!--/ Evaluation. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">&#128588 Examples</h2>
  
    <div class="column">
      <div class="content">
        <h2 class="title is-4">Annotating Process</h2>
        <div class="content has-text-justified">
          <p>
            When inputting a visual content, the two debaters (i.e. affirmative and negative debaters) engage in a two-round debate on whether the given content is harmful. The "judge" then makes the final decision based on the arguments of both sides. For detailed role definitions for each of the three agents, please refer to the appendix of the paper.
          </p>

        <div class="columns is-vcentered interpolation-panel">
          <div class="column has-text-centered">
            <img src="./static/images/debate.png"
                 class="image_ICL_samples-image"
                 alt="Image ICL sample examples."/>
            <p>An example of the debate annotation framework. Please note that the white rectangle masks serve as censorship, and are not included as inputs. <i>"Affirm."</i> and <i>"Neg."</i> stand for the affirmative and negative debaters, respectively.</p>
          </div>
        </div>
      </div>

    <h2 class="title is-4 is-centered">In-context Learning (ICL) Samples</h2>
    <div class="content is-centered">
      <p>
        Here are the in-context learning samples of the image/video annotator and the corresponding expected responses.
        The white rectangles simply serve as censorship, and are not included as input.
      </p>
    </div>

    <div class="columns is-centered interpolation-panel">
      <!-- Image -->
      <div class="column">
        <h2 class="title is-5">Image</h2>
        <div class=" has-text-centered">
          <img src="./static/images/image_ICL_samples_2.png"
               class="image_ICL_samples-image"
               alt="Image ICL sample examples."/>
        </div>
      </div>
      <!--/ Image -->

      <!-- Video. -->
      <div class="column">
        <h2 class="title is-5">Video</h2>
        <div class=" has-text-centered">
          <img src="./static/images/video_frame_ICL_samples.png"
               class="video_frame_ICL_samples-image"
               alt="Video frame ICL sample examples."/>
        </div>
      </div>
      <!--/ Video. -->
    </div>
  
  </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <!-- Acknowledgement. -->
  <div class="container is-max-desktop content">
    <h2 class="title is-3">&#128170 Acknowledgement</h2>

    <div class="content has-text-justified">
      <p>
        This project is built upon the the giant sholder of <a href="https://github.com/microsoft/autogen">Autogen</a>. Great thanks to them!
      </p>
    </div>
  </div>
  <!--/ Acknowledgement. -->

  <div class="container is-max-desktop content">
    <h2 class="title is-3">&#128080 BibTeX</h2>
    <pre><code>@inproceedings{yeh2024t2vs,
  author={Chen Yeh and You-Ming Chang and Wei-Chen Chiu and Ning Yu},
  booktitle={Advances in Neural Information Processing Systems},
  title={T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition},
  year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
